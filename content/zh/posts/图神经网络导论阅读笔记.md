---
author: "Me"
title: "图神经网络导论阅读笔记"
date: 2021-11-04T16:22:06+08:00
description: "A Gentle Introduction to Graph Neural Networks"
draft: false
hideToc: false
enableToc: true
enableTocContent: true
author: Me
authorEmoji: 🤖
tags: 
- 机器学习
- 图神经网络
libraries:
- mathjax
image: images/CuteColorIcons/icons8-mind-map-96.png
---
[连接](https://distill.pub/2021/gnn-intro/)
### 什么是图
图是一种表示实体和实体之间关系的数据结构。
图由节点、边和全局属性组成。
节点属性、边属性和全局属性都可以编码为向量。
根据边是否具有方向性，可将图分为有向图和无向图。

把图片表示为图：每个像素点可以认为是一个节点，每个节点的3个通道数表示为3维向量，边相邻和角相邻都认为是相邻。
把文本表示为图：每一个词表示为一个节点，每个词与下一个词之间是有向连接。
把分子表示为图：原子表示为节点，键表示为边。
把社交网络表示为图：人物表示为节点，人物关系表示为边。
把引用关系表示为图：文章表示为节点，引用关系表示为有向边。
#### 图上可以定义什么问题
问题包括3种层面：节点层面、边层面和图层面。
图层面的任务：例如图分类任务。
节点层面的任务：例如两节点亲和度比较，判断其余所有节点对这两个节点的亲和度，用于预测图的分裂。
边层面的任务：例如去预测节点之间应该是什么关系
#### 图机器学习面对的挑战
图一般有4种信息，点、边、全局和连接。前三种可以作为向量表示，这是深度学习常用的。连接关系可以用邻接矩阵表示但存在以下问题：
1. 矩阵规模太大
2. 稀疏矩阵处理困难
3. 交换矩阵的行列对于连接性是不变的

连接关系也可以用邻接列表表示，列表的长度与边的数量一致且先后次序无关
### 图神经网络
对于图的交换不变的特征，一个可以采用的方法是基于消息传递的图神经网络，采用图输入、图输出的IO方式，在内部对各种属性采取变换，但不会改变连接性。

一个最简单的GNN的一层是对节点、边和全局分别做一个MLP，这里没有用到结构信息。随后输出采用池化的方法：
1. 有自己属性的节点、边和全局预测直接MLP输出
2. 没有自己属性的节点，使用连接边和全局的属性汇总输出
3. 没有自己属性的边，使用连接节点和全局的属性汇总输出
4. 没有自己属性的全局，使用全部节点的属性汇总输出

消息传递：
在GNN层内部，
1. 节点的更新可以由相邻节点的属性汇聚变换而成，这种方式类似于CNN中的标准卷积，但卷积权重都为1
2. 也可以用节点信息更新边信息，再由边信息更新节点信息。向量可以相加，也可以拼接，通过MLP投影到初始的维度。但是先点后边与先边后点的更新顺序对于结果是有影响的，要做实验比较表现。
3. 另一种方式是同时节点汇聚到边，边汇聚到顶点，但不加入原顶点和原边的信息，形成中间信息，随后对中间信息再做一次汇聚，最后加入原边原顶点的信息完成更新。

全局属性表示的意义在于处理距离非常远的节点之间的消息传递，全局属性可视作一个主节点，与所有的边和所有的节点相连，参与节点和边的更新，由边和节点的更新而更新。
### 相关技术
#### 高级图
MultiGraph：一组顶点之间存在不同种类的边
HyperNodeGraph: 节点是一幅子图

#### 图采样
计算采样：对于处理超大图的情况，为了节省资源，可以选取一部分节点，只进行子图的计算和更新
输出采样：由于消息传递了很多层，如果图本身的连通性足够，任意一个节点都可以拿来输出。

节点与边的对偶性：可以相互转化
图的注意力机制：在于消除交换节点位置对神经网络的影响，更加注重节点的值。
