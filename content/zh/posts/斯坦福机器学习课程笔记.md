---
author: "Me"
title: "实用机器学习课程笔记"
date: 2021-11-01T15:28:06+08:00
description: "CS239P Pratical Machine Learning Directed by Mu Li"
draft: false
hideToc: false
enableToc: true
enableTocContent: true
author: Me
authorEmoji: 🤖
tags: 
- 机器学习
libraries:
- mathjax
image: images/CuteColorIcons/icons8-learning-96.png
---

课程视频的地址在[B站](https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358496)

### 1.2 数据获取

### 1.3 网页数据抓取

### 1.4 数据标注

是否有足够多的标注？ Yes 去训练一个半监督学习 No 下一步
是否可能增加标注？ Yes 人工打标签 No 使用弱标注及弱监督学习
是否真的需要标注？ No 尝试非监督学习和自监督学习

#### 半监督学习
半监督学习的假设：
1. 连续性假设：拥有相似特征的样本通常拥有相同的标号
2. 聚类假设：数据具有内在的聚类逻辑，相同类之内的数据可能具有相同的标号
3. 流形假设：数据集合位于一个低维的流形之上，内嵌于输入空间，即数据具有可降维性。

自学习是一种半监督学习的方法:
1. 用有标号的数据训练模型
2. 对未标号的数据进行预测
3. 保留高可信度的生成标号，加入下一轮的训练 （高可行度可以是分类意义上的可信度百分比，也可以是具体问题上由先验知识来判断的可信与否）

自学习的优势是能够使用贵的模型，例如大网络，模型集成之类。

#### 人工标注
降低人工标注成本的方法：
1. 降低标注任务的难度：做更加友好的用户界面，给予标注人员更加准确的任务引导
2. 减少标注任务的数量：主动学习

主动学习：
1. 将最感兴趣的，最复杂的样本给标注人员去标注
2. 将分类最不明确的样本送给人去标注
3. 主动学习与自学习常常混合使用，高可行度的直接加入标签，低可信度的去打标签，然后加入下一轮的训练

#### 弱监督
半自动地生成标注
数据编程：用启发式的方法生成标注，例如关键词搜索，规律总结，模式匹配，第三方模型评估等。

### 2.1 探索性数据分析
1. 丢掉超过30%的样本为缺失值的属性列
```python
null_sum = data.isnull().sum()
data.drop(columns=data.columns[null_sum > len(data)*0.3], inplace=True)
```
2. 修正数据类型不正确的属性列：用dataframe.replace()和正则表达式完成从string到float的转换
3. 处理异常的极值：配合直方图逐步清理
4. 初步可视化：概率分布图、箱图、协方差热力图，理解值的分布，理解属性列之间的相关性

### 2.2 数据清理
数据错误的类型：
1. 值超过了正确的区间，长尾的值，离群的值
2. 值应当唯一或值应当非空
3. 对齐、格式、拼写错误等

### 2.3 数据变换
归一化/Normalization：
1. Min(a)-Max(b)线性映射：$ \frac{x_i-\mathrm{min}_x}{\mathrm{max}_x-\mathrm{min}_x}(b-a)+a $
2. 标准正态化/Z-score(均值为0，方差为1)：$ \frac{x_i-\mathrm{mean}(x)}{\mathrm{std}(x)}$
3. Decimal Scaling: Slove j $  \frac{\mathrm{max}_x}{10^{\mathrm{min}_j}} < 1 $
4. Log Scaling: $ \mathrm{Log}(x_i)$

图片变换：
* 下采样、裁切、照片白化。
* 注意：降低jpeg品质会导致准确率下降， Midium 80%-90%的品质会导致1%准确率下降

视频变换：
1. 通常处理的片段在10秒以内，仅包含一个动作或事件
2. 只选取几个帧或只采样关键帧

文本变换：
1. 词根化、去语法化：转换为原型或词根
2. 词源化/Tokenization： 切到词(word)、字词(subword)、字母(char)， 中文分词

### 2.4 特征工程
表格数据：
1. 数值：直接使用或者分桶
2. 类别： One-hot编码
3. 日期时间：[Year, Month, Day, Day of year, Week of year, Day of week], 星期几也很重要，所以相对于每周的日期也必须有表示。
4. 复合类型：单一类型的笛卡尔积

文本数据：
1. Bag of Word(BoW)： 统计一句话中的词频，会丢掉词间语义信息
2. Word Embedding(Word2Vec): 词向量
3. 预训练模型的输出：Bert, GPT-3

图片和视频：
1. DL时代以前的算法：SIFT
2. 预训练模型的输出：ResNet 照片分类； I3D 动作分类

### 3.1 机器学习算法总论
机器学习分类：
1. 监督学习
    * 自监督学习
2. 半监督学习：E.g. 自训练
3. 无监督学习：E.g. 聚类，分布估计(GAN)
4. 强化学习：从环境中学习、交互与反馈

监督学习的组成：
1. 模型：输入到输出
2. 损失函数： 衡量预测与真实值的距离
3. 优化目标： 要去学习和提升的函数
4. 优化： 调整参数以达到优化目标

监督学习的常见模型算法：
1. 决策树
2. 线性模型
3. 核方法
4. 神经网络

### 3.2 决策树
分类树和回归树：叶子节点是类别还是数值

优点：
1. 可解释性好
2. 类别和数值特征都可以处理

缺点（及解决方法）：
1. 对噪声非常不稳定（集成为森林）
2. 复杂树是过拟合的（剪枝）
3. 不易并行计算

#### 随机森林
特点：
1. 用多棵树提升稳定性
2. 分类用投票，回归用平均
3. 随机性来源：
    1. Bagging: 随机重复采样，并替换其他样本 E.g. [1,2,3,4,5]->[1,2,2,3,4]
    2. 随机选择子样本集和子特征列

#### Gradient Boosting Decision Trees/梯度提升树
按次序训练多棵树，步骤是：
1. 前t棵树的输出之和为$F_t(x)$
2. 以$y-F_t(x)$残差为目标训练第t+1棵树
3. 前t+1棵树的输出之和为$F_{t+1}(x)=F_t(x)+f_{t+1}(x)$

树模型的优点还有：
1. 模型简单
2. 不太需要调参
3. 效果尚可

### 3.3 线性模型

### 3.4 随机梯度下降

### 3.5 多层感知机

### 3.6 卷积神经网络

### 3.7 循环神经网络
一般的MLP很难处理变长序列（BoW方法会丢失词序信息），也没有记忆过去输入信息的能力。
RNN的处理方法是将前一步输出前未经过softmax的向量作为隐藏态，与后一步的输入向量拼接然后输入网络。隐藏状态包含了过去所有输入的信息。

RNN和门控RNN:
简单RNN: $h_t=\phi(W_{hh}h_{t-1}+W_{hx}x_{t}+b_h)$
门控RNN(LSTM, GRU)：按照任务需求抑制输入($x_{t}$)或抑制过去($h_{t-1}$)，抑制的权重参数也是可学习的。

双向RNN和深层RNN：
双向RNN: 有正向层和反向层，正向层看之前时刻的输入，反向层看之后时刻的输入，输出由正向层反向层的输出叠加。只能用来做完形填空，无法做续写。
深层RNN: t时刻的输出输入之间有多个RNN层，单个RNN层可以是单向的也可以是双向的。

### 4.1 模型评估
常见的分类指标：
1. 准确率/Accuracy：正确预测数/总数  (真阳性+真阴性)/(真阳性+真阴性+假阳性+假阴性)
2. 精准率/Precition：正确预测的阳性数/预测为阳性的总数  (真阳性)/(真阳性+假阳性)
3. 召回率/Recall：正确预测的阳性数/实际为阳性的总数   (真阳性)/(真阳性+假阴性)
4. F1：平衡Precision和Recall，常见为$2pr/(p+r)$

AUC & ROC:
综合衡量模型的区分能力，计算步骤是：
1. 首先选取阈值，预测概率高于阈值则预测为阳性，预测概率低于阈值则预测为阴性。
2. ROC曲线的x轴为错误预测为阳性的阴性样本数/实际为阴性的总数((假阳性)/(真阴性+假阳性))，y轴为正确预测为阳性的阳性样本数/实际为阳性的总数((真阳性)/(真阳性+假阴性))，ROC曲线是一条关于阈值的参数方程曲线。
3. AUC为ROC曲线下的面积。AUC=1表示存在一个阈值能完美区分正负样本，AUC=0.5表示模型没有任何区分能力， AUC=0表示模型能将正负样本完全区分，但完全识别反了。



